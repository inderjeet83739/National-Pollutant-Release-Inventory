# -*- coding: utf-8 -*-
"""Npri_Classification_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZoS6ddmdfek3-wRsUw4KHgR0BkMi677I

### In this file, we have created our ML model to classify the total release in three different categories, high risk, mid risk and low risk.

# Npri Classification Model Documentation

## Project Overview
This project is part of a group assignment focused on predicting environmental impact using the **National Pollutant Release Inventory (NPRI) dataset**. The primary goal of Phase 1 is to **prepare the dataset for time series analysis and develop a classification model** to categorize pollutant releases.

## Phase 1: Time Series Data Preparation and Classification

### 1. Transform the Dataset
- The NPRI dataset was **restructured into a time series format** to ensure compatibility with machine learning models.
- Data transformation included:
  - **Timestamp Formatting:** Ensuring date/time fields are in a standard format.
  - **Handling Missing Values:** Addressed gaps through imputation and interpolation methods.
  - **Normalization:** Standardized features to maintain consistency in temporal analysis.

### 2. Develop a Classification Model
To gain insights and set the stage for **Phase 2 (Regression Analysis)**, we defined a classification task by:
- **Binning Continuous Pollutant Release Data**:
  - High Risk
  - Mid Risk
  - Low Risk
- **Model Selection:**
  - Initially implemented **Random Forest Classifier**.
  - Addressed overfitting through hyperparameter tuning (`n_estimators`, `max_depth`).
  - Evaluated performance using **accuracy, confusion matrix, and F1-score**.

### 3. Explain Your Approach
Each step was documented in a **Jupyter Notebook**, covering:
- **Data Transformation Choices**: Justification for restructuring and preprocessing methods.
- **Feature Engineering Techniques**: Selection of relevant attributes for classification.
- **Model Development**: Explanation of classification methodology and parameter tuning.
- **Visualizations**: Included only when necessary to support findings.

## Feature Importance Analysis
To determine the most influential features, we extracted and visualized feature importances using `matplotlib`. This helped in:
- Understanding which attributes contribute most to the classification.
- Reducing less important features for better efficiency.

## Model Evaluation
- **Accuracy Score**: Assessed the performance on test data.
- **Confusion Matrix**: Visualized model predictions.
- **Precision, Recall, and F1-Score**: Evaluated overall classification performance.

## Challenges Faced
- **Overfitting**: Mitigated through parameter tuning.
- **Imbalanced Data**: Handled by adjusting class weights.
- **Feature Correlation**: Some attributes were redundant, leading to model simplification.

## Future Improvements
- Implementing additional models (e.g., **Gradient Boosting**, **XGBoost**) for comparison.
- Exploring **Feature Engineering** to enhance model performance.
- Deploying the model using a simple API for real-world usability.

---

This documentation provides a structured overview of Phase 1 of the project. Additional refinements will be made as we proceed to **Phase 2 (Regression Analysis)**.

Random Forest Classifier (Overfitting)
  *	Problem: Too complex with deep trees memorizing the dataset.
  *	Why you didn’t choose it:
  *	Accuracy = 1.0 (no generalization).
  *	Possible data leakage (important feature directly correlating with the target).
  *	Model complexity made it sensitive to noise.
"""

# Re-import necessary libraries and reload dataset

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# Reload the dataset

df = pd.read_csv('cleaneddataset_NPRI-1.csv')

# Convert the 'Reporting_Year / Année' column into a proper datetime format
df['Reporting_Year'] = pd.to_datetime(df['Reporting_Year / Année'], format='%Y')

# Set the datetime column as the index
df.set_index('Reporting_Year', inplace=True)

# Selecting relevant time series columns for analysis
time_series_columns = ['total_releases', 'Air_spill', 'Land_spill', 'Water_spill']
df_time_series = df[time_series_columns]

# Define bins for pollutant release classification
bins = [0, 10, 50, 200, 1000, float('inf')]
labels = ['Very Low', 'Low', 'Moderate', 'High', 'Very High']

# Apply binning to 'total_releases' to create categorical classes
df['Pollutant_Class'] = pd.cut(df['total_releases'], bins=bins, labels=labels)

# Prepare Features and Target Variable
X = df_time_series  # Features: Time-series pollutant data
y = df['Pollutant_Class']  # Target: Categorical pollutant levels

# Encode the target variable
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)

# Define hyperparameter grid for RandomForestClassifier
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform Grid Search with Cross Validation
grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get best parameters
best_params = grid_search.best_params_

# Train the best model
best_classifier = RandomForestClassifier(**best_params, random_state=42)
best_classifier.fit(X_train, y_train)

# Make predictions with the tuned model
y_pred_tuned = best_classifier.predict(X_test)

# Evaluate the tuned model
accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
classification_rep_tuned = classification_report(y_test, y_pred_tuned, target_names=label_encoder.classes_)

# Display results
best_params, accuracy_tuned, classification_rep_tuned

# Reduce Overfitting by tuning the model with simpler settings

# Define a new hyperparameter grid with reduced complexity
optimized_param_grid = {
    'n_estimators': [30, 50],  # Reduce number of estimators
    'max_depth': [5, 7],  # Reduce tree depth to prevent overfitting
    'min_samples_split': [5, 10],  # Increase the minimum samples to split
    'min_samples_leaf': [2, 4]  # Increase the minimum leaf size
}

# Perform Grid Search with Cross Validation on the optimized parameters
grid_search_optimized = GridSearchCV(RandomForestClassifier(random_state=42),
                                     optimized_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search_optimized.fit(X_train, y_train)

# Get best parameters from the optimized grid search
best_params_optimized = grid_search_optimized.best_params_

# Train the optimized model
optimized_classifier = RandomForestClassifier(**best_params_optimized, random_state=42)
optimized_classifier.fit(X_train, y_train)

# Make predictions with the optimized model
y_pred_optimized = optimized_classifier.predict(X_test)

# Evaluate the optimized model
accuracy_optimized = accuracy_score(y_test, y_pred_optimized)
classification_rep_optimized = classification_report(y_test, y_pred_optimized, target_names=label_encoder.classes_)

# Display results
best_params_optimized, accuracy_optimized, classification_rep_optimized

# Perform feature importance analysis using the trained Random Forest model

import matplotlib.pyplot as plt
import numpy as np

# Get feature importances from the optimized model
feature_importances = optimized_classifier.feature_importances_

# Create a DataFrame for visualization
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importance
plt.figure(figsize=(10, 5))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
plt.xlabel("Feature Importance")
plt.ylabel("Features")
plt.title("Feature Importance in Random Forest Classifier")
plt.gca().invert_yaxis()  # Invert axis for better readability
plt.show()

# Check correlations between features
correlation_matrix = X.corr()

pip install pandas numpy scikit-learn

""" Now Using Logistic Regression"""

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the dataset
file_path = "cleaneddataset_NPRI-1.csv"  # Replace with the actual path to your dataset
df = pd.read_csv(file_path)

# Rename the reporting year column if necessary
df.rename(columns={'Reporting_Year / Année': 'reporting_year'}, inplace=True)

# Define the target column and feature columns
target_column = 'spill_risk_category'
categorical_columns = [
    'Company_Name / Dénomination_sociale_de_l\'entreprise',
    'Substance Name (English) / Nom de substance (Anglais)',
    'NAICS Title / Titre Code_SCIAN'
]
numerical_columns = ['total_releases', 'Air_spill', 'Land_spill', 'Water_spill']

# Ensure the target column exists (apply classification logic if missing)
if target_column not in df.columns:
    # Define fixed thresholds for risk classification
    low_risk_threshold = 10
    high_risk_threshold = 50

    def classify_risk_fixed(count):
        if count >= high_risk_threshold:
            return 'High-risk'
        elif count >= low_risk_threshold:
            return 'Medium-risk'
        else:
            return 'Low-risk'

    df[target_column] = df['total_releases'].apply(classify_risk_fixed)

# Encode the target variable
label_encoder = LabelEncoder()
df[target_column] = label_encoder.fit_transform(df[target_column])

# Perform stratified train-test split since we only have data for 2000
X = df[categorical_columns + numerical_columns]
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Define preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_columns),
        ('cat', categorical_pipeline, categorical_columns)
    ]
)

# Apply transformations
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Train the Logistic Regression model
logistic_model = LogisticRegression(max_iter=200, random_state=42)
logistic_model.fit(X_train_processed, y_train)

# Make predictions on the test set
y_pred = logistic_model.predict(X_test_processed)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred, target_names=label_encoder.classes_)

# Print results
print(f"Model Accuracy: {accuracy:.4f}")
print("\nClassification Report:\n", classification_rep)

# Re-import necessary libraries after execution reset
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Reload the dataset
file_path = "cleaneddataset_NPRI-1.csv"
df = pd.read_csv(file_path)

# Rename the reporting year column if necessary
df.rename(columns={'Reporting_Year / Année': 'reporting_year'}, inplace=True)

# Define the target column and feature columns
target_column = 'spill_risk_category'
categorical_columns = [
    'Company_Name / Dénomination_sociale_de_l\'entreprise',
    'Substance Name (English) / Nom de substance (Anglais)',
    'NAICS Title / Titre Code_SCIAN'
]
numerical_columns = ['total_releases', 'Air_spill', 'Land_spill', 'Water_spill']

# Ensure the target column exists (apply classification logic if missing)
if target_column not in df.columns:
    # Define fixed thresholds for risk classification
    low_risk_threshold = 10
    high_risk_threshold = 50

    def classify_risk_fixed(count):
        if count >= high_risk_threshold:
            return 'High-risk'
        elif count >= low_risk_threshold:
            return 'Medium-risk'
        else:
            return 'Low-risk'

    df[target_column] = df['total_releases'].apply(classify_risk_fixed)

# Encode the target variable
label_encoder = LabelEncoder()
df[target_column] = label_encoder.fit_transform(df[target_column])

# Perform stratified train-test split since we only have data for 2000
X = df[categorical_columns + numerical_columns]
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Define preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_columns),
        ('cat', categorical_pipeline, categorical_columns)
    ]
)

# Apply transformations
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Train a Logistic Regression model
logistic_model = LogisticRegression(max_iter=200, random_state=42)
logistic_model.fit(X_train_processed, y_train)

# Perform Hyperparameter Tuning using GridSearchCV for Logistic Regression
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength
    'class_weight': [None, 'balanced']  # Class weight options
}

# Initialize GridSearchCV
grid_search = GridSearchCV(logistic_model, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=1)

# Perform Grid Search on the training set
grid_search.fit(X_train_processed, y_train)

# Get the best parameters and best estimator from GridSearch
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Make predictions with the tuned model
y_pred_tuned = best_model.predict(X_test_processed)

# Evaluate the tuned model
accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
classification_rep_tuned = classification_report(y_test, y_pred_tuned, target_names=label_encoder.classes_)

# Display best parameters and results
best_params, accuracy_tuned, classification_rep_tuned

# Import RandomizedSearchCV for faster hyperparameter tuning
from sklearn.model_selection import RandomizedSearchCV

# Define the parameter distribution for Logistic Regression
param_dist = {
    'C': np.logspace(-2, 2, 10),  # Logarithmic range of regularization strength
    'class_weight': [None, 'balanced']  # Class weight options
}

# Initialize Logistic Regression model
log_reg = LogisticRegression(max_iter=200, random_state=42)

# Set up RandomizedSearchCV with 10 iterations
random_search = RandomizedSearchCV(log_reg, param_distributions=param_dist,
                                   n_iter=10, cv=5, scoring='f1_weighted',
                                   n_jobs=-1, verbose=1, random_state=42)

# Perform Randomized Search on the training set
random_search.fit(X_train_processed, y_train)

# Get the best parameters and best estimator from RandomizedSearch
best_params_random = random_search.best_params_
best_model_random = random_search.best_estimator_

# Make predictions with the tuned model
y_pred_random = best_model_random.predict(X_test_processed)

# Evaluate the tuned model
accuracy_random = accuracy_score(y_test, y_pred_random)
classification_rep_random = classification_report(y_test, y_pred_random, target_names=label_encoder.classes_)

# Display best parameters and results
best_params_random, accuracy_random, classification_rep_random

# Rename reporting year column if necessary
df.rename(columns={'Reporting_Year / Année': 'reporting_year'})

# Define target and feature columns
target_column = 'spill_risk_category'
categorical_columns = [
    'Company_Name / Dénomination_sociale_de_l\'entreprise',
    'Substance Name (English) / Nom de substance (Anglais)',
    'NAICS Title / Titre Code_SCIAN'
]
numerical_columns = ['total_releases', 'Air_spill', 'Land_spill', 'Water_spill']

# Ensure the target column exists
if target_column not in df.columns:
    low_risk_threshold = 10
    high_risk_threshold = 50
    def classify_risk_fixed(count):
        if count >= high_risk_threshold:
            return 'High-risk'
        elif count >= low_risk_threshold:
            return 'Medium-risk'
        else:
            return 'Low-risk'
    df[target_column] = df['total_releases'].apply(classify_risk_fixed)

# Encode the target variable
df[target_column] = df[target_column].astype(str)
df[target_column] = label_encoder.fit_transform(df[target_column])

# Train-test split (Stratified)
X = df[categorical_columns + numerical_columns]
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Define preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))  # Ensure sparse output
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_columns),
        ('cat', categorical_pipeline, categorical_columns)
    ]
)

# Apply transformations
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Define a Logistic Regression pipeline with Reduced Polynomial Complexity
log_reg_poly_reduced = make_pipeline(
    PolynomialFeatures(degree=2),  # Full polynomial expansion
    StandardScaler(with_mean=False),  # Fix sparse matrix issue
    LogisticRegression(max_iter=300, C=100, solver='saga', random_state=42)
)

# Train the fine-tuned Logistic Regression model
log_reg_poly_reduced.fit(X_train_processed, y_train)

# Make predictions
y_pred_poly_reduced = log_reg_poly_reduced.predict(X_test_processed)

# Evaluate the fine-tuned model
accuracy_poly_reduced = accuracy_score(y_test, y_pred_poly_reduced)
classification_rep_poly_reduced = classification_report(y_test, y_pred_poly_reduced, target_names=label_encoder.classes_)

# Display results
print(f"Reduced Polynomial Logistic Regression Accuracy: {accuracy_poly_reduced:.4f}")
print("\nClassification Report:\n", classification_rep_poly_reduced)

# Update logistic regression pipeline to fix convergence
log_reg_poly_fixed = make_pipeline(
    PolynomialFeatures(degree=2),  # Full polynomial expansion
    StandardScaler(with_mean=False),  # Fix sparse matrix issue
    LogisticRegression(max_iter=500, C=10, solver='lbfgs', random_state=42)  # Increased max_iter and changed solver
)

# Train the updated model
log_reg_poly_fixed.fit(X_train_processed, y_train)

# Make predictions
y_pred_poly_fixed = log_reg_poly_fixed.predict(X_test_processed)

# Evaluate the improved model
accuracy_poly_fixed = accuracy_score(y_test, y_pred_poly_fixed)
classification_rep_poly_fixed = classification_report(y_test, y_pred_poly_fixed, target_names=label_encoder.classes_)

# Display results
print(f"Improved Logistic Regression Accuracy: {accuracy_poly_fixed:.4f}")
print("\nClassification Report:\n", classification_rep_poly_fixed)

"""Testing different models"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load dataset
file_path = "cleaneddataset_NPRI-1.csv"  # Update with actual file path
df = pd.read_csv(file_path)

# Rename reporting year column if necessary
df.rename(columns={'Reporting_Year / Année': 'reporting_year'})

# Define target and feature columns
target_column = 'spill_risk_category'
categorical_columns = [
    'Company_Name / Dénomination_sociale_de_l\'entreprise',
    'Substance Name (English) / Nom de substance (Anglais)',
    'NAICS Title / Titre Code_SCIAN'
]
numerical_columns = ['total_releases', 'Air_spill', 'Land_spill', 'Water_spill']

# Ensure the target column exists
if target_column not in df.columns:
    low_risk_threshold = 10
    high_risk_threshold = 50
    def classify_risk_fixed(count):
        if count >= high_risk_threshold:
            return 'High-risk'
        elif count >= low_risk_threshold:
            return 'Medium-risk'
        else:
            return 'Low-risk'
    df[target_column] = df['total_releases'].apply(classify_risk_fixed)

# Encode the target variable
label_encoder = LabelEncoder()
df[target_column] = df[target_column].astype(str)  # Convert to string before encoding
df[target_column] = label_encoder.fit_transform(df[target_column])

# Train-test split (Stratified)
X = df[categorical_columns + numerical_columns]
y = df[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Define preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))  # Ensure sparse output
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_columns),
        ('cat', categorical_pipeline, categorical_columns)
    ]
)

# Apply transformations
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# ==========================
# Train Logistic Regression
# ==========================
log_reg = LogisticRegression(max_iter=500, C=10, solver='lbfgs', random_state=42)
log_reg.fit(X_train_processed, y_train)
y_pred_logreg = log_reg.predict(X_test_processed)
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
classification_rep_logreg = classification_report(y_test, y_pred_logreg, target_names=label_encoder.classes_)

# ==========================
# Train Random Forest
# ==========================
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_model.fit(X_train_processed, y_train)
y_pred_rf = rf_model.predict(X_test_processed)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
classification_rep_rf = classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_)

# ==========================
# Train XGBoost
# ==========================
xgb_model = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train_processed, y_train)
y_pred_xgb = xgb_model.predict(X_test_processed)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
classification_rep_xgb = classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_)

# ==========================
# Display results
# ==========================
print("\n==== Logistic Regression ====")
print(f"Accuracy: {accuracy_logreg:.4f}")
print(classification_rep_logreg)

print("\n==== Random Forest ====")
print(f"Accuracy: {accuracy_rf:.4f}")
print(classification_rep_rf)

print("\n==== XGBoost ====")
print(f"Accuracy: {accuracy_xgb:.4f}")
print(classification_rep_xgb)

pip install pandas numpy scikit-learn

"""###Why Logistic Regression Worked

* After testing Random Forest and XGBoost, both models overfitted the data, achieving an unrealistic accuracy of 1.0. This indicated that they were too complex and memorized the training data instead of learning general patterns.

* Logistic Regression worked better because it is a simpler model that generalizes well. It does not have the complexity of tree-based models, which reduced the risk of overfitting. The model was also improved through proper preprocessing, including scaling numerical features, encoding categorical features, and handling missing values.

* Another reason Logistic Regression performed well was the use of regularization (C=10), which helped balance model flexibility and prevent overfitting. Additionally, using a stratified train-test split ensured that all categories were fairly represented, making the model more reliable.

* As a result, Logistic Regression achieved an accuracy of 0.97, providing a good balance between performance and generalization.
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the real-world dataset
file_path = "cleaneddataset_NPRI-2.csv"  # Update with actual file path
df_real = pd.read_csv(file_path)

# Rename reporting year column if necessary
df_real.rename(columns={'Reporting_Year / Année': 'reporting_year'},)

# Define target and feature columns
target_column = 'spill_risk_category'
categorical_columns = [
    'Company_Name / Dénomination_sociale_de_l\'entreprise',
    'Substance Name (English) / Nom de substance (Anglais)',
    'NAICS Title / Titre Code_SCIAN'
]
numerical_columns = ['total_releases', 'Air_spill', 'Land_spill', 'Water_spill']

# Ensure the target column exists
if target_column not in df_real.columns:
    low_risk_threshold = 10
    high_risk_threshold = 50
    def classify_risk_fixed(count):
        if count >= high_risk_threshold:
            return 'High-risk'
        elif count >= low_risk_threshold:
            return 'Medium-risk'
        else:
            return 'Low-risk'
    df_real[target_column] = df_real['total_releases'].apply(classify_risk_fixed)

# Encode the target variable
label_encoder = LabelEncoder()
df_real[target_column] = df_real[target_column].astype(str)  # Convert to string before encoding
df_real[target_column] = label_encoder.fit_transform(df_real[target_column])

# Train-test split (Stratified)
X = df_real[categorical_columns + numerical_columns]
y = df_real[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y)

# Define preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))  # Ensure sparse output
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_columns),
        ('cat', categorical_pipeline, categorical_columns)
    ]
)

# Apply transformations
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=500, C=10, solver='lbfgs', random_state=42)
log_reg.fit(X_train_processed, y_train)

# Make predictions
y_pred_logreg = log_reg.predict(X_test_processed)

# Evaluate the model
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
classification_rep_logreg = classification_report(y_test, y_pred_logreg, target_names=label_encoder.classes_)

# Display results
print("\n==== Logistic Regression on Real-World Data ====")
print(f"Accuracy: {accuracy_logreg:.4f}")
print(classification_rep_logreg)

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Load the real-world dataset
file_path = "cleaneddataset_NPRI.csv"  # Update with actual file path

# Rename reporting year column if necessary
df_real.rename(columns={'Reporting_Year / Année': 'reporting_year'}, inplace=True)

# Define target and feature columns
target_column = 'spill_risk_category'
categorical_columns = [
    'Company_Name / Dénomination_sociale_de_l\'entreprise',
    'Substance Name (English) / Nom de substance (Anglais)',
    'NAICS Title / Titre Code_SCIAN'
]
numerical_columns = ['total_releases', 'Air_spill', 'Land_spill', 'Water_spill']

# Ensure the target column exists
if target_column not in df_real.columns:
    low_risk_threshold = 10
    high_risk_threshold = 50
    def classify_risk_fixed(count):
        if count >= high_risk_threshold:
            return 'High-risk'
        elif count >= low_risk_threshold:
            return 'Medium-risk'
        else:
            return 'Low-risk'
    df_real[target_column] = df_real['total_releases'].apply(classify_risk_fixed)

# Encode the target variable
label_encoder = LabelEncoder()
df_real[target_column] = df_real[target_column].astype(str)  # Convert to string before encoding
df_real[target_column] = label_encoder.fit_transform(df_real[target_column])

# Train-test split (Stratified)
X = df_real[categorical_columns + numerical_columns]
y = df_real[target_column]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

# Define preprocessing pipelines
numerical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
    ('scaler', StandardScaler())
])

categorical_pipeline = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))  # Ensure sparse output
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_pipeline, numerical_columns),
        ('cat', categorical_pipeline, categorical_columns)
    ]
)

# Apply transformations
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Train Logistic Regression
log_reg = LogisticRegression(max_iter=500, C=10, solver='lbfgs', random_state=42)
log_reg.fit(X_train_processed, y_train)

# Make predictions
y_pred_logreg = log_reg.predict(X_test_processed)

# Evaluate the model
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
classification_rep_logreg = classification_report(y_test, y_pred_logreg, target_names=label_encoder.classes_)

# Display results
print("\n==== Logistic Regression on Real-World Data ====")
print(f"Accuracy: {accuracy_logreg:.4f}")
print(classification_rep_logreg)